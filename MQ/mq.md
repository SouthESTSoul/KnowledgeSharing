![image-20210413032021016](https://i.loli.net/2021/05/19/kAE1jcZDCOoQVlM.png)

多个系统需要同一个方法，





## 项目里是怎么用消息队列的？

### 



## 为什么使用消息队列？

### //todo

### 使用消息队列的主要作用是：异步、解耦、削峰

项目里是怎么用来异步削峰和解耦的？

只要发送到消息队列就行了，不用每次加一个cartridge的时候，再更新代码。

解耦的例子：

![image-20200418213021225](https://i.loli.net/2021/05/19/RljtrEiQo2S6fgd.png)

![image-20200419205127214](https://i.loli.net/2021/05/19/ZJzeC2yv3AgTijY.png)



异步和削峰的例子？



## 消息队列都有什么优缺点？



### MQ带来的问题：

可用性降低：MQ自己挂了，导致下游全部收不到

复杂性提高：重复消费，消息丢失，消费顺序，一致性问题。



### 怎么解决重复消费？

这个问题可以变成如何实现幂等？

#### 重复消费的场景：

kafka实际上有个offset的概念，就是每个消息写进去，都有一个offset，代表他的序号，然后consumer消费了数据之后，每隔一段时间，会把自己消费过的消息offset提交一下，代表我已经消费过了，下次我要是重启啥的，你就让我从上次消费到的offset来继续消费。

就是你有时候重启系统，看你怎么重启，如果碰到着急的，直接kill杀死进程，然后重启，这就会导致consumer有些消息处理了没来得及提交offset，然后重启后，就会造成少数消息重复消费的问题。

消费者如果在准备提交offset，但是还没有提交的时候，消费者进程被重启，那么此时已经消费过数据的offset并没有提交，kafka也就不知道你已经消费了，那么消费者再次上线进行消费的时候，会把已经消费的数据，重新在传递过来，这就是消息重复消费的问题。

#### 怎么保证消息队列消费的幂等？

- 如果用的是redis，那就没问题了，因为每次都是set操作，天然的幂等性
- 如果不是上面的，那就做的稍微复杂一点，需要让生产者发送每条消息的时候，需要加一个全局唯一的id，类似于订单id之后的东西，然后你这里消费到了之后，先根据这个id去redis中查找，之前消费过了么，如果没有消费过，那就进行处理，然后把这个id写入到redis中，如果消费过了，那就别处理了，保证别重复消费相同的消息即可。
- 还有比如基于数据库唯一键来保证重复数据不会重复插入多条，我们之前线上系统就有这个问题，就是拿到数据的时候，每次重启可能会重复，因为Kafka消费者还没来得及提交offset，重复数据拿到了以后，我们进行插入的时候，因为有了唯一键约束了，所以重复数据只会插入报错，不会导致数据库中出现脏数据。



### 怎么解决消息丢失？

这个问题可以理解为如何保证消息的可靠性传输？

#### 消息丢失的场景

丢数据，一般分为两种，要么是MQ自己弄丢了，要么是我们消费的时候弄丢了。我们可以从RabbitMQ和Kafka分别来进行分析。

#### RabbitMQ的场景

生产者以为写到rabbitmq里了，但是没有存上去。

rabbitmq存在内存里的数据，还没持久化到磁盘，rabbitmq挂了。

消费者拉到了这个数据，还没处理但是消费者挂了。

![image-20200420120701475](https://i.loli.net/2021/05/19/TjxvzOmbWG1arEi.png)

#### 生产者丢数据的解决方案：

事务：此时选择用RabbitMQ提供的事务功能，就是生产者发送数据之前，开启RabbitMQ事务（channel.txSelect），然后发送消息，此时就可以回滚事务（channel.txRollback），然后重试发送消息，如果收到了消息，那么可以提交事务，但是问题是，RabbitMQ事务机制一搞，基本上吞吐量会下来，因为太损耗性能。

异步回调：所以一般来说，如果你要确保写RabbitMQ消息别丢，可以开启confirm模式，在生产者那里设置了开启confirm模式之后，RabbitMQ会给你回传一个ack消息，告诉你这个消息OK了，如果RabbitMQ没能处理这个消息，会给你回调一个接口，告诉你这个消息接收失败，你可以重试，

一般生产者如果要保证消息不丢失，一般是用confirm机制，因为是异步的模式，在发送消息之后，不会阻塞，直接可以发送下一条消息，这样吞吐量会更高一些。

- 收到消息ack之后，从kv存储里删除这条临时消息，收到nack后，就重新投递。

Kafka设置了对应的参数后，生产者不会丢数据。

##### 如果MQ宕机，怎么保证数据不丢失？（双缓存冲机制）

- 可以将数据先写到存储中（可以是数据库。也可以是其他），等MQ恢复再进行消费
- 可以将数据写到本地磁盘，此时可以进行优化
  - 先写到缓冲区，等缓冲区满释放锁执行写磁盘操作
  - 切换到另一个缓冲区进行写，业务没有中断。
  - 写磁盘的时候不需要加锁，减少锁持有的时间
  - ![image-20210515225956490](https://i.loli.net/2021/05/19/ZQ8wUD631eduPcB.png)





#### RabbitMQ自己丢数据：

这个就是RabbitMQ自己丢失数据，这个时候就必须开启RabbitMQ的持久化，就是消息写入之后，同时需要持久化到磁盘中，哪怕是RabbitMQ自己宕机了，也能够从磁盘中读取之前存储的消息，这样数据一般就不会丢失了，但是存在一个极端的情况，就是RabbitMQ还没持久化的时候，就已经宕机了，那么可能会造成少量的数据丢失，但是这个概率是比较小的。

可以和生产者的异步回调结合，持久化到磁盘才会通知生产者ACK了。	

#### Kafka 自己丢数据：

设置对应的参数，每个partition至少有2个副本；每条数据必须写入所有的replica后才能写入成功；如果写入失败就无限重试。

如果设置了这些参数，可以看到生产者不会丢数据。

#### 消费者丢数据：

消费者丢失数据，主要是因为打开了AutoAck的机制，消费者会自动通知RabbitMQ，表明自己已经消费完这条数据了，但是如果你消费到了一条消息，还在处理中，还没处理完，此时消费者就会自动AutoAck了，通知RabbitMQ说这条消息已经被消费了，此时不巧的是，消费者系统宕机了，这条消息就会丢失，因为RabbitMQ以为这条消息已经处理掉。

在消费者层面上，我们需要将AutoAck给关闭，然后每次自己确定已经处理完了一条消息后，你再发送ack给RabbitMQ，如果你还没处理完就宕机了，此时RabbitMQ没收到你发的Ack消息，然后RabbitMQ就会将这条消息分配给其它的消费者去处理。



### 如何保证消费的顺序性？

在mysql里增删改一条数据，对应出来的增删改3条binlog，接着这三条binlog发送到MQ里面，到消费出来依次执行，这个时候起码得保证能够顺序执行，不然本来是：增加、修改、删除，然后被换成了：删除、修改、增加，不全错了呢。

#### 顺序不一致的场景：

在消息队列中，一个queue中的数据，一次只会被一个消费者消费掉，但因为不同消费者的执行速度不一致，在存入数据库后，造成顺序不一致的问题。

#### RabbitMQ保证消费顺序性：

把需要保证顺序的数据发到一个Queue里，然后让一个消费者来消费，那么这个消费者消费的时候就是顺序执行的了。

但是如果消费者中处理的时候是多线程处理的呢？看看kafka的方案！

![image-20200420151354856](https://i.loli.net/2021/05/19/W179BUgrqVeKiv2.png)

#### Kafka保证消费顺序性：

一个topic，一个partition，一个consumer，内部单线程消费，写N个内存，然后N个线程分别消费一个内存queu即可

如果是多线程消费，那么通过hash分发，将相同订单的key散列到同一个内存队列里，然后每一个线程从这个Queue中拉数据，同一个内存Queue也是有顺序的。

![image-20200420153622880](https://i.loli.net/2021/05/19/rQN2IjiaUg1TZvm.png)



## 怎么解决消息积压？

### 消息大量积压

假设1个消费者1秒消费1000条，1秒3个消费者能消费3000条，一分钟就是18万条，1000万条也需要花费1小时才能够把消息处理掉，这个时候在设备允许的情况下，如何才能够快速处理积压的消息呢？

- 先修复consumer的问题，确保其恢复消费速度，然后将现有consumer都停止
- 临时建立好原先10倍或者20倍的queue数量
- 然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue
- 接着临时征用10倍机器来部署consumer，每一批consumer消费一个临时queue的数据
- 这种做法相当于临时将queue资源和consumer资源扩大了10倍，以正常的10倍速度

### 消息大量积压且有过期时间

就不是说要增加consumer消费积压的消息，因为实际上没有啥积压的，而是丢了大量的消息，我们可以采取的一个方案就是，批量重导，这个之前线上也有遇到类似的场景，就是大量的消息积压的时候，然后就直接丢弃了数据，然后等高峰期过了之后，例如在晚上12点以后，就开始写程序，将丢失的那批数据，写个临时程序，一点点查询出来，然后重新 添加MQ里面，把白天丢的数据，全部补回来。

假设1万个订单积压在MQ里面，没有处理，其中1000个订单都丢了，你只能手动写程序把那1000个订单查询出来，然后手动发到MQ里面去再补一次。

### 大量积压消息，导致MQ磁盘满了

与前面两者结合一下，分发到10倍的queue数量里，半夜再重新填入到MQ里消费。



## MQ是怎么保证高可用的？

### RabbitMQ的高可用

RabbitMQ 三种模式：单机模式，普通集群模式，镜像集群模式

单机模式:demo级别。

#### 普通集群：

意思就是在多台机器上启动多个RabbitMQ实例，每台机器启动一个，但是创建的Queue，只会放在一个RabbitMQ实例上，但是每个实例都同步queue元数据，在消费的时候，实际上是连接到另外一个实例上，那么这个实例会从queue所在实例上拉取数据过来，这种方式确实很麻烦，也不怎么好，没做到所谓的分布式 ，就是个普通集群。因为这导致你要么消费每次随机连接一个实例，然后拉取数据，要么固定连接那个queue所在实例消费数据，前者有数据拉取的开销，后者导致单实例性能瓶颈。

- 可能会在RabbitMQ中存在大量的数据传输
- 可用性没有什么保障，如果queue所在的节点宕机，就会导致queue的消息丢失

![image-20200420091806944](https://i.loli.net/2021/05/19/gTsp1YPZokBtQ7L.png)

#### 镜像集群模式

这种模式，才是RabbitMQ的高可用模式，你创建的queue无论元数据还是queue里的消息都会存在与多个实例中，然后每次你写消息到queu的时候，都会自动把消息推送到多个实例的queue中进行消息同步。

这样的好处在于，你任何一个机器宕机了，别的机器都可以用。坏处在于，性能开销提升，消息同步所有的机器，导致网络带宽压力和消耗增加，第二就是没有什么扩展性科研，如果某个queue负载很重，你加机器，新增的机器也包含了这个queue的所有数据，并没有办法线性扩展你的queue

那么如何开启集群镜像策略呢？就是在RabbitMQ的管理控制台，新增一个策略，这个策略就是镜像集群模式下的策略，指定的时候，可以要求数据同步到所有的节点，也可以要求就 同步到指定数量的节点，然后再次创建queue的时候，应用这个策略，就会自动将数据同步到其它节点上去了

但是这个模式还存在问题：就是不是分布式的，如果这个queue的数据量很大，大到这个机器上的容量无法容纳的时候，此时应该怎么办呢？

![image-20200420102752707](https://i.loli.net/2021/05/19/8AcElNSDqzLgFTv.png)

### Kafka的高可用

kafka0.8以前，是没有HA机制的，就是任何一个broker宕机了，那个broker上的partition就废了，没法读也没办法写，没有什么高可用可言，而在0.8版本后，提供了HA机制，就是replica副本机制，每个partition的数据都会同步到其它机器上，形成自己的多个replica副本，然后所有的replica就是follower，写的时候，leader会负责数据都同步到所有的follower上，读的时候就直接读取leader上的数据即可。只能读写leader？很简单，要是你能随意读写每个follower，那么就需要保证数据一致性的问题，系统复杂度太高，很容易出问题，kafka会均匀的将一个partition的所有replica分布在不同的机器上，这样才能够提高容错性

此时，高可用的架构就出来了，假设现在某个机器宕机了，比如其中的一个leader宕机了，但是因为每个leader下还有多个follower，并且每个follower都进行了数据的备份，因此kafka会自动感知leader已经宕机，同时将其它的follower给选举出来，作为新的leader，并向外提供服务支持。

可用性高：replicate + isr + 选举 机制保证；

特别注意，kafka中的分区只能被一个消费组中的一个消费者消费。一个消费者可以消费多个partition

举个例子，partition1能被A 消费者组中的消费者1消费，还能被B消费者组中的消费者1消费，这两个组中的不同消费者可以指向不同的游标

![image-20210519224926430](https://i.loli.net/2021/05/19/uTEa7IQBlpeVstZ.png)

## 怎么去设计一个MQ？

- 首先MQ得支持可伸缩性，那就需要快速扩容，就可以增加吞吐量和容量，可以设计一个分布式的系统，参考kafka的设计理念，broker - > topic -> partition，每个partition放一台机器，那就存一部分数据，如果现在资源不够了，可以给topic增加partition，然后做数据迁移，增加机器，不就可以存放更多的数据，提高更高的吞吐量。
  - 怎么做数据迁移？新增broker节点只有新创建的partition才会被分到这里来，可以从各个leader拷贝一份数据，形成新的broker再做一些写设置
- 其次得考虑一下这个MQ的数据要不要落地磁盘？也就是需不需要保证消息持久化，因为这样可以保证数据的不丢失，那落地盘的时候怎么落？顺序写，这样没有磁盘随机读写的寻址开销，磁盘顺序读的性能是很高的，这就是kafka的思路。
  - 用MMAP落地磁盘，用sendfile发送到网卡，都是零拷贝技术。
- 其次需要考虑MQ的可用性？这个可以具体到我们上面提到的消息队列保证高可用，提出了多副本 ，leader 和follower模式，当一个leader宕机的时候，马上选取一个follower作为新的leader对外提供服务。
- 需不需要支持数据0丢失？可以参考kafka零丢失方案
  - 三个参数，备份数量，落地磁盘，重试次数



## Kafka是怎么存储日志的？

### //TODO

## Kafka有什么不足的地方？

- 无法弹性扩容：对partition的读写都在partition leader所在的broker，如果该broker压力过大，也无法通过新增broker来解决问题；
- 扩容成本高：集群中新增的broker只会处理新topic，如果要分担老topic-partition的压力，需要手动迁移partition，这时会占用大量集群带宽；
- 消费者新加入和退出会造成整个消费组rebalance：导致数据重复消费，影响消费速度，增加e2e延迟；
- partition过多会使得性能显著下降：ZK压力大，broker上partition过多让磁盘顺序写几乎退化成随机写。

在了解了kafka的架构之后，你可以仔细想一想，为什么kafka扩容这么费劲呢？其实这本质上和redis集群扩容是一样的！当redis集群出现热key时，某个实例扛不住了，你通过加机器并不能解决什么问题，因为那个热key还是在之前的某个实例中，新扩容的实例起不到分流的作用。

可以联想一下mysql的分表。比如用户订单表，由于量太大把它按用户id拆分成1024个子表user_order_{0..1023}，如果到后期发现还不够用，要增加这个分表数，就会比较麻烦。因为分表总数增多，会让user_id的hash值发生变化，从而导致老的数据无法查询。所以只能停服做数据迁移，然后再重新上线。

## 各个MQ的对比

| 特性       | ActiveMQ                                                     | RabbitMQ                                                     | RocketMQ                                                     | Kafka                                                        |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 单机吞吐量 | 万级，吞吐量比RocketMQ和Kafka要低一个数量级                  | 万级，吞吐量比RocketMQ和Kafka要低一个数量级                  | 10万级，RocketMQ也是可以支撑高吞吐的一种MQ                   | 10万级1这是kafka最大的优点，就是吞吐量高。一般配置和数据类的系统进行实时数据计算、日志采集等场景 |
| 时效性     | ms级                                                         | 微妙级，这是RabbitMQ的一大特点，就是延迟最低                 | ms级                                                         | 延迟在ms级内                                                 |
| 可用性     | 基于主从架构实现高可用                                       | 高，基于主从架构实现高可用                                   | 非常高，分布式架构                                           | 非常高，kafka是分布式的，一个数据多个副本，少数机器宕机后，不会丢失数据，不会导致不可用 |
| 消息可靠性 | 有较低的概率丢失数据                                         | 消息不丢失                                                   | 经过参数优化配置，可以做到0丢失                              | 经过参数优化配置可以做到0丢失                                |
| 核心特点   | MQ领域的功能及其完备                                         | 基于Erlang开发，所以并发能力强，性能及其好，延时很低         | MQ功能较为完善，还是分布式的，扩展性好                       | 功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用，是实时上的标准。 |
|            | 非常成熟，功能强大，在业内大量公司以及项目都有应用。 但是偶尔消息丢失的概率，并且现在社区以及国内应用都越来越少，官方社区对ActiveMQ5.X维护越来越少，而且确实主要是基于解耦和异步来用的，较少在大规模吞吐场景中使用 | erlang语言开发的，性能及其好，延时很低。而且开源的版本，就提供的管理界面非常棒，在国内一些互联网公司近几年用RabbitMQ也是比较多一些，特别适用于中小型的公司 缺点显而易见，就是吞吐量会低一些，这是因为它做的实现机制比较中，因为使用erlang开发，目前没有多少公司使用其开发。所以针对源码界别的定制，非常困难，因此公司的掌控非常弱，只能依赖于开源社区的维护。 | 接口简单易用，毕竟在阿里大规模应用过，有阿里平台保障，日处理消息上 百亿之多，可以做到大规模吞吐，性能也非常好，分布式扩展也很方便，社区维护还可以，可靠性和可用性都是OK的，还可以支撑大规模的topic数量，支持复杂MQ业务场景。 | 仅仅提供较少的核心功能，但是提供超高的吞吐量，ms级别的延迟，极高的可用性以及可靠性，分布式可以任意扩展。 同时kafka最好是支撑较少的topic数量即可，保证其超高的吞吐量。 |

综上所述：

- 一般的业务要引入MQ，最早大家都是用ACviceMQ，但是现在大家用的不多了，没有经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了，不太图鉴使用
- RabbitMQ后面被大量的中小型公司所使用，但是erlang语言阻碍了大量的Java工程师深入研究和掌握它，对公司而言，几乎处于不可控的状态，但是RabbitMQ目前开源稳定，活跃度也表较高。
- RocketMQ是阿里开源的一套消息中间件，目前也已经经历了天猫双十一，同时底层使用Java进行开发

如果中小型企业技术实力一般，技术挑战不是很高，可以推荐，RabbitMQ。如果公司的基础研发能力很强，想精确到源码级别的掌握，那么推荐使用RocketMQ。同时如果项目是聚焦于大数据领域的实时计算，日志采集等场景，那么Kafka是业内标准。





https://www.bilibili.com/video/BV1FE411y79Y?p=20

https://gitee.com/moxi159753/LearningNotes/tree/master/%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95/%E9%9D%A2%E8%AF%95%E6%89%AB%E7%9B%B2%E5%AD%A6%E4%B9%A0/1_%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E9%9D%A2%E8%AF%95%E8%BF%9E%E7%8E%AF%E7%82%AE#%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%E4%BB%A5%E5%8F%8A%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF



https://mp.weixin.qq.com/s?__biz=MzU2OTY0MjcyOQ==&mid=2247488566&idx=1&sn=e94f46247e733c071a47a7b97fe2dee1&chksm=fcfacfb0cb8d46a62fd4f6c34034f14325067a49fbdd9a67268c63f524a2f1fc5d191476e49a&mpshare=1&scene=23&srcid=0418sYFEKSMj4MQvnJ7y0C4q&sharer_sharetime=1618719059600&sharer_shareid=0defeb502992f9d6a3d386c65b142674#rd

